{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test baseline accuracy MATLAB VS Python data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TKH---------------\n",
      "-Result of Python Filter  : (0.6187499999999999, 0.054126587736527385, array([0.58333333, 0.7       , 0.55833333, 0.63333333]))\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f40ae4de9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4040241940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "-Result of Matlab Filter  : (0.6041666666666666, 0.06495190528383289, array([0.66666667, 0.65      , 0.6       , 0.5       ]))\n",
      "--------------------SGYM---------------\n",
      "-Result of Python Filter  : (0.6291666666666667, 0.037960139913101244, array([0.66666667, 0.56666667, 0.63333333, 0.65      ]))\n",
      "-Result of Matlab Filter  : (0.6458333333333333, 0.032004773949452545, array([0.6       , 0.63333333, 0.66666667, 0.68333333]))\n",
      "--------------------LYF---------------\n",
      "-Result of Python Filter  : (0.5125, 0.037499999999999985, array([0.575     , 0.50833333, 0.48333333, 0.48333333]))\n",
      "-Result of Matlab Filter  : (0.5708333333333333, 0.04768967975941498, array([0.63333333, 0.58333333, 0.56666667, 0.5       ]))\n",
      "--------------------SGR---------------\n",
      "-Result of Python Filter  : (0.4604166666666667, 0.05962120008855912, array([0.475     , 0.39166667, 0.425     , 0.55      ]))\n",
      "-Result of Matlab Filter  : (0.5708333333333333, 0.05448623679425839, array([0.51666667, 0.61666667, 0.51666667, 0.63333333]))\n",
      "CPU times: user 6min 29s, sys: 1min 16s, total: 7min 46s\n",
      "Wall time: 7min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from model_set import EEGNet, SugiyamaNet\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import lfilter, butter\n",
    "\n",
    "import capilab_dataset2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "ALL_SUBJS = ['Takahashi', 'Lai', 'Sugiyama', 'Suguro']\n",
    "\n",
    "\n",
    "def load1(subj):    \n",
    "    fs = 500\n",
    "    duration = 2\n",
    "    sample = fs * duration\n",
    "    ch = 19\n",
    "    hp = 0.5\n",
    "    lp = 40\n",
    "    #data1, label1 = capilab_dataset2.load_target('Takahashi_JulyData')\n",
    "    #data2, label2 = capilab_dataset2.load_target('Suguro_JulyData')\n",
    "    #data3, label3 = capilab_dataset2.load_target('Lai_JulyData')\n",
    "    #data4, label4 = capilab_dataset2.load_target('Sugiyama_JulyData')\n",
    "    #data = np.vstack([data1, data2, data3, data4])\n",
    "    data, label = capilab_dataset2.load_target(subj + '_JulyData')\n",
    "    #label = np.vstack([label1, label2, label3, label4])\n",
    "    try:\n",
    "        x, x_test,  y, y_test = train_test_split(data, label, test_size = .2, stratify = label, random_state = 1)\n",
    "        x = capilab_dataset2.butterworth_bpf(x, hp, lp, fs)\n",
    "        x_test = capilab_dataset2.butterworth_bpf(x_test, hp, lp, fs)\n",
    "        x = np.expand_dims(x, axis = 3)\n",
    "        x_test = np.expand_dims(x_test, axis = 3)\n",
    "        # # swap sample and channels axis\n",
    "        # x = np.transpose(x, (0,2,1,3))\n",
    "        # x_test = np.transpose(x_test, (0,2,1,3))\n",
    "        \n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print(message)\n",
    "        return None\n",
    "    else:\n",
    "        return x, y, x_test, y_test\n",
    "\n",
    "def load2(target_file):    \n",
    "\n",
    "    try:\n",
    "        contents= loadmat(target_file)\n",
    "        X = contents['raw_x']\n",
    "        Y = contents['raw_y']\n",
    "\n",
    "        #target shape = (data, 19, 1000, 1), from (1000, 19, data)\n",
    "        X = np.transpose(X, [2,1,0])\n",
    "        Y = np.transpose(Y, [1,0])\n",
    "        X = np.expand_dims(X, axis = 3)\n",
    "        x, x_test,  y, y_test = train_test_split(X, Y, test_size = .1, stratify = Y, random_state = 1)\n",
    "    except Exception as ex:\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print(message)\n",
    "        return None\n",
    "    else:\n",
    "        return x, y, x_test, y_test\n",
    "    \n",
    "def _step(X, y, x_val, y_val, x_test, y_test, verbose = False):\n",
    "    tf.keras.utils.set_random_seed(1)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    classifier_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    classifier_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    clf = SugiyamaNet(Chans = X.shape[1], Samples = X.shape[2], output_classes = y.shape[1])\n",
    "    # clf = EEGNet(y.shape[1], Chans = X.shape[1], Samples = X.shape[2], \n",
    "                                            # dropoutRate = 0.5, kernLength = 512, F1 = 64, \n",
    "                                            # D = 8, F2 = 128, norm_rate = 0.25, dropoutType = 'Dropout')\n",
    "    \n",
    "    clf.compile(optimizer = classifier_optimizer, loss= classifier_loss , metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=False, save_best_only=True)\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4, verbose=0)\n",
    "    clf.fit(X, y,\n",
    "            batch_size=8, \n",
    "            epochs = 12, \n",
    "            verbose = verbose, \n",
    "            validation_data = (x_val, y_val),\n",
    "            callbacks = [checkpointer, earlystopper])\n",
    "    y_preds = clf.predict(x_test, verbose = verbose)\n",
    "    predicted = np.argmax(y_preds, axis=1)\n",
    "    ground_truth = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    r = accuracy_score(ground_truth, predicted)\n",
    "    # clf.save('temp_model')\n",
    "    return r\n",
    "\n",
    "def __train1(subj_name = None, b = 8, ep = 12):\n",
    "\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    x, y, x_test,y_test = load1(subj_name) #DATA\n",
    "\n",
    "    # print(x.shape)  \n",
    "    r = np.array([])\n",
    "    kfold = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 0) #fold\n",
    "    for i , (train, val) in enumerate(kfold.split(x, np.argmax(y, axis = 1))):\n",
    "        # tf.random.set_seed(0)\n",
    "        res = _step(x[train], y[train], x[val], y[val],x_test, y_test)\n",
    "        r = np.append(r, res)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    return r.mean(), r.std(), r\n",
    "\n",
    "def __train2(subj_name = None, b = 8, ep = 12):\n",
    "\n",
    "\n",
    "    f1 = []\n",
    "    acc = []\n",
    "    x, y, x_test,y_test = load2(subj_name) #DATA\n",
    "\n",
    "    # print(x.shape)  \n",
    "    r = np.array([])\n",
    "    kfold = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 0) #fold\n",
    "    for i , (train, val) in enumerate(kfold.split(x, np.argmax(y, axis = 1))):\n",
    "        # tf.random.set_seed(0)\n",
    "        res = _step(x[train], y[train], x[val], y[val],x_test, y_test)\n",
    "        r = np.append(r, res)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    return r.mean(), r.std(), r\n",
    "\n",
    "\n",
    "print(\"--------------------TKH---------------\")\n",
    "print(f'-Result of Python Filter  : {__train1(\"Takahashi\")}')\n",
    "print(f'-Result of Matlab Filter  : {__train2(\"Datasets/tkh_filtered_data.mat\")}')\n",
    "print(\"--------------------SGYM---------------\")\n",
    "print(f'-Result of Python Filter  : {__train1(\"Sugiyama\")}')\n",
    "print(f'-Result of Matlab Filter  : {__train2(\"Datasets/sgym_filtered_data.mat\")}')\n",
    "print(\"--------------------LYF---------------\")\n",
    "print(f'-Result of Python Filter  : {__train1(\"Lai\")}')\n",
    "print(f'-Result of Matlab Filter  : {__train2(\"Datasets/lyf_filtered_data.mat\")}')\n",
    "print(\"--------------------SGR---------------\")\n",
    "print(f'-Result of Python Filter  : {__train1(\"Suguro\")}')\n",
    "print(f'-Result of Matlab Filter  : {__train2(\"Datasets/sgr_filtered_data.mat\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test filter\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import lfilter, butter\n",
    "fs = 500\n",
    "durations = 2\n",
    "sample = fs * durations\n",
    "ch = 19\n",
    "\n",
    "def butterworth_bandpass(low, high, fs, order = 4):\n",
    "    return butter(order, [low, high], fs = fs, btype = 'band', output = 'ba', analog = False)\n",
    "\n",
    "def butterworth_bpf(data, low, high, fs, order = 4):\n",
    "    b,a = butterworth_bandpass(low, high, fs, order)\n",
    "    return lfilter(b,a, data, axis = 1)\n",
    "\n",
    "f = 'Datasets/targets.mat'\n",
    "contents = loadmat(f)\n",
    "\n",
    "target1 = contents['target1'] #1000 x 19 correct \n",
    "target2 = contents['target2'] #19 x 1000 incorrect\n",
    "matlab_filt = np.transpose(contents['filtered'], [1,0])\n",
    "filtered = butterworth_bpf(target2, 0.4, 40, 500 /2)\n",
    "\n",
    "print(target1.shape, target2.shape, matlab_filt.shape, filtered.shape)\n",
    "\n",
    "sim = np.subtract(filtered, matlab_filt)\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "fig, ax = plt.subplots(4)\n",
    "\n",
    "ax[0].set_title(\"raw\")\n",
    "ax[0].plot(target1)\n",
    "\n",
    "ax[1].set_title(\"python filter\")\n",
    "ax[1].plot(filtered.transpose())\n",
    "\n",
    "ax[2].set_title(\"matlab filter\")\n",
    "ax[2].plot(matlab_filt.transpose())\n",
    "\n",
    "ax[3].set_title(\"diff\")\n",
    "ax[3].plot(sim.transpose())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "#test weigth copy\n",
    "train_model = Sequential([\n",
    "    Flatten(),             \n",
    "    Dense(units=32,activation = 'relu'),\n",
    "    Dense(units=64,activation = 'relu'),\n",
    "    Dense(units=128,activation = 'relu'),\n",
    "    Dense(4, activation = 'softmax')\n",
    "])\n",
    "train_model.compile(loss=\"huber\", optimizer = Adam(learning_rate = 1e-3))\n",
    "train_model.build((1,19))\n",
    "target_model = Sequential([\n",
    "    Flatten(),             \n",
    "    Dense(units=32,activation = 'relu'),\n",
    "    Dense(units=64,activation = 'relu'),\n",
    "    Dense(units=128,activation = 'relu'),\n",
    "    Dense(4, activation = 'softmax')\n",
    "])\n",
    "target_model.compile(loss=\"huber\", optimizer = Adam(learning_rate = 1e-3))\n",
    "target_model.build((1,19))\n",
    "\n",
    "target_model.set_weights(train_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  42%|██████████████████▉                          | 421/1000 [07:44<10:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  44%|███████████████████▊                         | 441/1000 [08:06<10:18,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  46%|████████████████████▋                        | 461/1000 [08:28<09:56,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  48%|█████████████████████▋                       | 481/1000 [08:51<09:33,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  50%|██████████████████████▌                      | 501/1000 [09:13<09:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  52%|███████████████████████▍                     | 521/1000 [09:35<08:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  54%|████████████████████████▎                    | 541/1000 [09:57<08:27,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  56%|█████████████████████████▏                   | 561/1000 [10:19<08:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  58%|██████████████████████████▏                  | 581/1000 [10:41<07:43,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  60%|███████████████████████████                  | 601/1000 [11:03<07:21,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  62%|███████████████████████████▉                 | 621/1000 [11:25<06:59,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  64%|████████████████████████████▊                | 641/1000 [11:47<06:37,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  66%|█████████████████████████████▋               | 661/1000 [12:09<06:14,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  68%|██████████████████████████████▋              | 681/1000 [12:31<05:52,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  70%|███████████████████████████████▌             | 701/1000 [12:53<05:30,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  72%|████████████████████████████████▍            | 721/1000 [13:16<05:08,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  74%|█████████████████████████████████▎           | 741/1000 [13:38<04:46,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  76%|██████████████████████████████████▏          | 761/1000 [14:00<04:24,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  78%|███████████████████████████████████▏         | 781/1000 [14:22<04:02,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  80%|████████████████████████████████████         | 801/1000 [14:44<03:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  82%|████████████████████████████████████▉        | 821/1000 [15:06<03:18,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  84%|█████████████████████████████████████▊       | 841/1000 [15:28<02:55,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  86%|██████████████████████████████████████▋      | 861/1000 [15:50<02:33,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  88%|███████████████████████████████████████▋     | 881/1000 [16:12<02:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  90%|████████████████████████████████████████▌    | 901/1000 [16:34<01:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  92%|█████████████████████████████████████████▍   | 921/1000 [16:56<01:27,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  94%|██████████████████████████████████████████▎  | 941/1000 [17:18<01:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  96%|███████████████████████████████████████████▏ | 961/1000 [17:41<00:43,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>:  98%|████████████████████████████████████████████▏| 981/1000 [18:03<00:21,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train di wa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN Training>: 100%|████████████████████████████████████████████| 1000/1000 [18:24<00:00,  1.10s/it]\n",
      "100%|██████████| 1000/1000 [18:24<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "total_steps = 0\n",
    "good_exp = 0\n",
    "bad_exp = 0\n",
    "\n",
    "with tqdm(total = 1000, position = 0, leave = True) as pbar:\n",
    "    for e in tqdm(range(1000), ncols = 100, position = 0, leave = True, desc =\"DQN Training>\"):\n",
    "        for i in range(11):\n",
    "            if e % 69 == 0:\n",
    "                good_exp += 1\n",
    "            else:\n",
    "                bad_exp +=1\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        if good_exp > 69 and bad_exp > 69:\n",
    "            #train\n",
    "            if total_steps %20 == 0:\n",
    "                tqdm.write(\"Train di wa\")    \n",
    "            \n",
    "        total_steps += 1\n",
    "        pbar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ray')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22e305b8758fb24c4206001533016ec76ae0b87b6dfb4f2294b295dd10d5ecfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
